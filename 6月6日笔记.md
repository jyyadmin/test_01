## 1、请求模块

### 1、python2:  urllib  urllib2

​      python3: 合并  urllib.request

### 2、常用方法

#### 1、urllib.request.urlopen(url)

作用：向网站发起请求并获得响应对象

格式： 字节流=request.read().decode(utf-8)

缺点：不支持重构User-Agent

```python
import urllib.request

url = "https://www.baidu.com/"

response=urllib.request.urlopen(url)  #对网页发出请求
http=response.read().decode("utf-8")
print(html)
```

#### 2、urllib.request.Request(url)

作用：创建请求对象

##### 1、创建请求对象

erq=urllib.request.Request(url,headers)

##### 2、发送请求获取响应

response=urllib.request.urlopen(erq)

##### 3、获取响应内容

html=response.read().decode("utf-8")

```python
import urllib.request

url = "https://www.baidu.com/"
headers={"User-Agent": "Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36"}

# response=urllib.request.urlopen(url)  #对网页发出请求
# http=response.read().decode("utf-8")
# print(html)

#创建请求对象
req = urllib.request.Request(url=url,headers=headers)
#发送请求获取响应
response = urllib.request.urlopen(req)
#获取响应内容
html = response.read().decode("utf-8")
print(html)
```



## 2、url编码模块

### 1、模块名

urllib.parse

### 2、urlencode({字典})

("wwed":"将怨言")

```python
import urllib.parse

key={"web":"将怨言"}
data=urllib.parse.urlencode(key)
print(data)
```

### 3、quote方法

```python
import urllib.parse

key={"web":"将怨言"}
data=urllib.parse.urlencode(key)
data1=urllib.parse.quote("将怨言")
print(urllib.parse.unquote(data))
print(data)
print(data1)
```



**练习：**
编写一个爬虫程序，输入指定的贴吧，爬取指定的爬取页数,爬取相应的内容，使用面向对象编程

程序如下：

```python
import urllib.request
import urllib.parse

class baiduSpider():
    def __init__(self,url):
        self.baseurl=url
        self.headers={"User-Agent": "Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36"}

    def get_page(self,url):
        req=urllib.request.Request(url=url,headers=self.headers)
        res=urllib.request.urlopen(req)
        html=res.read().decode("utf-8")
        return html

    def work_on(self):
        name=input("请输入爬取的贴吧名称：")
        begin=int(input("请输入爬取的起始页："))
        end=int(input("请输入爬取的终止页："))
        key={"kw":name} #设置编码的字典格式
        key=urllib.parse.urlencode(key)  #进行url编码
        for page in range(begin,end+1):
            pn=50*(page-1)
            url=self.baseurl+key+"&pn"+str(pn) #拼接得到真实的url
            html=self.get_page(url)  #获取网页源代码
            filename="第"+str(page)+"页.html" # 创建文件名
            self.write_data(filename,html)  #调用写入方法写入数据

    def write_data(self,filename,html):
        with open(filename,"w",encoding="utf-8") as f:
            f.write(html)

url="http://tieba.baidu.com/f?"
baidu=baiduSpider(url)
baidu.work_on()
```










