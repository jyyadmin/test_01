### 1、数据的分类

#### 1、结构化数据

特点：固定格式HTML json xml

#### 2、非结构化数据

图片 音频 视频 一般存储为二进制

### 2、正则表达式re

#### 1、语法

result=re.search(r"表达式",''字符串',re.s)

#### 2、写法

#### 1、创建翻译对象

p=re.complice(r,e,s)

#### 2、进行字符串匹配

result=p.search(html)

### 3、search方法

1、re.search()方法用于扫描整个字符串并返回第一个成功匹配的字符串

2、如果匹配成功，re.search()返回一个匹配对象，否则返回none

re.search(pattern,sreing,flags)

pattern：匹配的正则表达式

string：要匹配的字符串

flage：标志位，用于控制正则表达式的匹配方式，如：是否区分大小写，多行匹配等

```python
import re

string=" This is a apple this is a orgen"
p=r" this"  #编写正则表达式
result=re.search(p,string,re.I)  #不区分大小写
result1=re.search(p,string)
print(result.group())
print(result1.group())
```

### 4、正则表达式中的元字符

| 元字符 | 作用                           |
| ------ | ------------------------------ |
| .      | 匹配任意一个字符串（不包括\n） |
| \d     | 一个数字                       |
| \s     | 空白字符                       |
| \S     | 非空白字符                     |
| []     | 包含[]内容                     |
| \w     | 普通字符                       |
| \W     | 特殊字符                       |
| *      | 0次或者n次                     |
| +      | 1次或n次                       |
| ？     | 0次或1次                       |
| {m}    | m次                            |

### 5、贪婪匹配和非贪婪匹配

1、贪婪匹配(.*)：在整个表达式匹配成功的前提下，尽可能多的匹配

2、非贪婪匹配(.*?)：在整个表达式匹配成功的前提下，尽可能少的匹配

```python
import re

string="<div>test</div><div>test2</div>"
p=re.compile("<div>.*</div>") #贪婪匹配
p1=re.compile("<div>.*?</div>")  #非贪婪匹配
result=p.search(string)
result1=p1.search(string)
print(result)
print(result1)
```

### 6、分组问题

在网页中，你想要什么数据，你就加括号

先按照整体的正则表达式出来，然后在匹配括号中的

如果有两个或者两个以上的括号，会形成元组显示

```python
import re

string="xxIxxdsdhxxamxxxwoqeiwxxcxkxx"
p=re.compile("xx(.*)xx")  #贪婪匹配
p1=re.compile("xx(.*?)xx")  #非贪婪匹配
result=p.findall(string)
result1=p1.findall(string)
print(result)
print(result1)
```

### 7、csv模块

#### 1、导入模块

import csv

#### 2、打开csv文件

with open ("xx.csv","w","newline=","encoding="utf-8") as f:

一定要加入newline="，否则会出现空行

#### 3、初始化写入数据

cs=csv.writer(f,dialect="excel")

#### 4、写入数据

cs.writerow(列表)

**例子如下：**

```python
import urllib.request
import re
import csv

class My_maoyan():
    def __init__(self):
        self.baseurl =  "https://maoyan.com/board/4?offset="
        self.headers = {"User-Agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.80 Safari/537.36"}
        self.offset = 0

    def get_html(self,url):
        req = urllib.request.Request(url=url, headers=self.headers)  # 创建请求对象
        res = urllib.request.urlopen(req)
        html = res.read().decode("utf-8")
        self.re_data(html)


    def re_data(self,html):
        p = re.compile(
            '<div class="movie-item-info".*?title="(.*?)".*?class="star">(.*?)</p>.*?class="releasetime">(.*?)</p>',
            re.S)  #正则表达式
        result = p.findall(html)
        self.writer_csv(result)  #调用上层函数获取结果数据

    def change_page(self):
        begin=int(input("请输入爬取起始页："))
        end=int(input("请输入爬取终止页："))
        for page in range(begin,end+1):
            self.offset=(page-1)*10   #更改offset的参数
            url=self.baseurl+str(self.offset)  #拼接成完整的url
            self.get_html(url)  #调用上层函数获取网页源代码

    def writer_csv(self,result):
        with open("movie1.csv","a",newline="",encoding="gbk") as f:
            cs = csv.writer(f, dialect="excel")  # 初始化写入对象
            for i in result:
                name=i[0].strip()
                star=i[1].strip()
                time=i[2].strip()
                content=[name,star,time]
                cs.writerow(content)  #写入数据
            print("数据保存成功！")

movie=My_maoyan()
movie.change_page()



```



### 